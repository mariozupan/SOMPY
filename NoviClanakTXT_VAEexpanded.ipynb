{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "NoviClanakTXT-VAEexpanded.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BlCCRipcW3tK"
      ],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariozupan/SOMPY/blob/master/NoviClanakTXT_VAEexpanded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShZfQtlJW3my",
        "colab_type": "raw"
      },
      "source": [
        "https://github.com/NicGian/text_VAE/blob/master/text_VAE_v18.ipynb\n",
        "\n",
        "https://nicgian.github.io/text-generation-vae/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiKC1UWyZLvt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zt0Lg6sXtz-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7508c722-d936-4db5-9b6a-29a724a06941"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjuZy0swo4j7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f5ee03ea-cefb-4514-b06e-06b3bfe01e15"
      },
      "source": [
        "'''\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "#And there you go. This allows you to access a free GPU for up to 12 hours at a time.\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport tensorflow as tf\\ndevice_name = tf.test.gpu_device_name()\\nif device_name != '/device:GPU:0':\\n  raise SystemError('GPU device not found')\\nprint('Found GPU at: {}'.format(device_name))\\n#And there you go. This allows you to access a free GPU for up to 12 hours at a time.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWPoqhR5ZFfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "afbb7b42-414c-4f51-c9b3-d81621b394ec"
      },
      "source": [
        "#!pip install gdown\n",
        "#!pip install tensorflow-gpu\n",
        "#po defaultu dobijes 12GB ram, ako zelis 25 onda\n",
        "'''\n",
        "a = []\n",
        "while(1):\n",
        "    a.append(‘1’)\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\na = []\\nwhile(1):\\n    a.append(‘1’)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syiq4RPpW3m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from scipy import spatial\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vthHD67hW3na",
        "colab_type": "text"
      },
      "source": [
        "We set the maximum sequence length to 25, the maximun number of words in our vocabulary to 20000 and we will use 300-dimensional embeddings. Finally we load our texts from a csv. The text file is the train file of the Quora Kaggle challenge containing around 808000 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1nEJetSW3n0",
        "colab_type": "raw"
      },
      "source": [
        "We will use pretrained Glove word embeddings as embeddings for our network. We create a matrix with one embedding for every word in our vocabulary and then we will pass this matrix as weights to the keras embedding layer of our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-jPV4ByZXCE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n77-e6GNZ0MA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83yl38D-W3n8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "VAE model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euVnrP-yW3n_",
        "colab_type": "text"
      },
      "source": [
        "##########################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ph5GqH8W3oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# prepare input data\n",
        "def prepare_inputsMoj(X):\n",
        "    X_enc = list()\n",
        "    le = list()\n",
        "\n",
        "    # label encode each column\n",
        "    for i in range(X.shape[1]):\n",
        "        le.append(LabelEncoder())\n",
        "        le[i].fit(X[:, i])\n",
        "        # encode\n",
        "        _enc = le[i].transform(X[:, i])\n",
        "        # store\n",
        "        X_enc.append(_enc)\n",
        "    return X_enc, le"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WA1z73hW3om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inverse_inputsMoj(X_enc, le):\n",
        "\tX_encInverse = list()\n",
        "\t# label encode each column\n",
        "\tfor i in range(X_enc.shape[1]):\n",
        "\t\t# encode\n",
        "\t\t_enc = le[i].inverse_transform(X_enc[:, i])\n",
        "\t\t# store\n",
        "\t\tX_encInverse.append(_enc)\n",
        "\treturn X_encInverse"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQIl0-eIW3pD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#funkcija za uređenje dataseta\n",
        "def urediDataset(df):\n",
        "\n",
        "    #koristi copy kad ne zelis: After this anything you do \n",
        "    #to df2 affects only df2 and not df1 and vice versa.\n",
        "    dfNew = df.copy()\n",
        "    #df['epoch'] = pd.to_datetime(df['DATUM'])    \n",
        "    #df['epoch'] = (df['epoch']-dt.datetime(1970,1,1)).dt.total_seconds()   \n",
        "\n",
        "    # IZNOSkod u posebnu kolonu\n",
        "    dfNew['IZNOSkod'] = dfNew[['DUGUJE','POTRAZUJE']].apply(lambda x: x['DUGUJE'] if x['DUGUJE']!=0 else x['POTRAZUJE'], axis=1)\n",
        "\n",
        "\n",
        "    #skrati konto\n",
        "    #df['KONTO'] = df['KONTO'].str[0:2].astype('category') #bilo je category\n",
        "\n",
        "\n",
        "\n",
        "    #duguje potrazuje u posebunu kolonu Dkod Pkod\n",
        "    dfNew['Dkod'] = dfNew[['DUGUJE']].apply(lambda x: 1 if x['DUGUJE']!=0 else 0, axis=1)\n",
        "    dfNew['Pkod'] = dfNew[['POTRAZUJE']].apply(lambda x: 1 if x['POTRAZUJE']!=0 else 0, axis=1)\n",
        "\n",
        "\n",
        "    # evo ga one_hot_encoded\n",
        "    #df = pd.get_dummies(df, columns=[\"DOKUMENT\", \"KONTO\", \"DIN\"], prefix=[\"DOKUMENT\", \"KONTO\", \"DIN\"])\n",
        "\n",
        "    #SKALIRAJ KOLONU DUGUJE i POTRAZUJR\n",
        "    skaliranje = MinMaxScaler(copy=True, feature_range=(0,1))\n",
        "\n",
        "    #df['DUGUJE'] = skaliranje.fit_transform(df[['DUGUJE']].values.reshape(-1,1))\n",
        "    #df['POTRAZUJE'] = skaliranje.fit_transform(df[['POTRAZUJE']].values.reshape(-1,1))\n",
        "    dfNew[\"IZNOSkod\"]  = skaliranje.fit_transform(dfNew['IZNOSkod'].values.reshape(-1,1))\n",
        "\n",
        "    dfNew = dfNew.reindex()\n",
        "\n",
        "    return dfNew"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b_qsb_nW3pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# filename=\"./data/txt/dekotex2007.txt\"\n",
        "# dataSirovi = pd.read_csv(filename, delimiter=\"\\t\", decimal=\",\",\n",
        "#                                                   usecols=[3,6,10,11], \n",
        "#                                                   encoding = 'utf8')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYZEPAza3b3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1036063d-af3f-457a-d59d-7e16953204c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwGg2sMB6gGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls \"/content/drive/My Drive\"\n",
        "\n",
        "##then copy file(s) as needed:\n",
        "#!cp \"/content/drive/My Drive/xy.py\" \"xy.py\"\n",
        "\n",
        "##confirm that files were copied:\n",
        "\n",
        "#!ls"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpHIjXhvW3pi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "54b742d9-8694-497e-ef7f-4a93d9f88736"
      },
      "source": [
        "dfVelicinaTrain=0\n",
        "dfVelicinaValidate=0\n",
        "dfVelicinaTest=0\n",
        "dfLista = list()\n",
        "for i in range(2007, 2020):\n",
        "    filename = '/content/drive/My Drive/dataGL/dekotex'+str(i)+\".txt\"\n",
        "    df = pd.read_csv(filename, delimiter=\"\\t\", decimal=\",\",\n",
        "                                              usecols=[3,6,10,11], \n",
        "                                              encoding = 'cp1252') #cp1252 utf8\n",
        "    dfLista.append(df)\n",
        "    print(str(i)+\". \")\n",
        "    print(df.shape[0])\n",
        "    \n",
        "    #izmjeri velicine dataset-ova\n",
        "    if (i<=2017):\n",
        "      dfVelicinaTrain = dfVelicinaTrain + df.shape[0]\n",
        "    elif (i==2018):\n",
        "      dfVelicinaValidate = dfVelicinaValidate + df.shape[0]\n",
        "    elif(i==2019):\n",
        "      dfVelicinaTest = dfVelicinaTest + df.shape[0]\n",
        "\n",
        "dataSirovi = pd.concat(dfLista)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2007. \n",
            "8011\n",
            "2008. \n",
            "7469\n",
            "2009. \n",
            "6660\n",
            "2010. \n",
            "6813\n",
            "2011. \n",
            "7103\n",
            "2012. \n",
            "7413\n",
            "2013. \n",
            "7049\n",
            "2014. \n",
            "7115\n",
            "2015. \n",
            "7540\n",
            "2016. \n",
            "6921\n",
            "2017. \n",
            "6544\n",
            "2018. \n",
            "6121\n",
            "2019. \n",
            "3864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqWQvTPFEmpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "21dee0b4-0888-48a1-db9b-24a4e19dc58a"
      },
      "source": [
        "print('velicina train '+str(dfVelicinaTrain))\n",
        "print('velicina validate '+str(dfVelicinaValidate))\n",
        "print('velicina test '+str(dfVelicinaTest))\n",
        "print('velicina ukupno '+str(dfVelicinaTrain+dfVelicinaValidate+dfVelicinaTest))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "velicina train 78638\n",
            "velicina validate 6121\n",
            "velicina test 3864\n",
            "velicina ukupno 88623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhtMGT9j6JS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4lntguXGW3pq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ed00957c-63c2-4928-db1a-ca47468b91ca"
      },
      "source": [
        "dataNorm = urediDataset(dataSirovi)\n",
        "dataNorm.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DOKUMENT</th>\n",
              "      <th>KONTO</th>\n",
              "      <th>DUGUJE</th>\n",
              "      <th>POTRAZUJE</th>\n",
              "      <th>IZNOSkod</th>\n",
              "      <th>Dkod</th>\n",
              "      <th>Pkod</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ACCOPENING</td>\n",
              "      <td>012</td>\n",
              "      <td>60383.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.227795</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACCOPENING</td>\n",
              "      <td>019</td>\n",
              "      <td>0.00</td>\n",
              "      <td>56184.56</td>\n",
              "      <td>0.225081</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ACCOPENING</td>\n",
              "      <td>020-000</td>\n",
              "      <td>88817.76</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.246168</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACCOPENING</td>\n",
              "      <td>021-000</td>\n",
              "      <td>8482.09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.194257</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ACCOPENING</td>\n",
              "      <td>021-004</td>\n",
              "      <td>3344.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.190937</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     DOKUMENT    KONTO    DUGUJE  POTRAZUJE  IZNOSkod  Dkod  Pkod\n",
              "0  ACCOPENING      012  60383.75       0.00  0.227795     1     0\n",
              "1  ACCOPENING      019      0.00   56184.56  0.225081     0     1\n",
              "2  ACCOPENING  020-000  88817.76       0.00  0.246168     1     0\n",
              "3  ACCOPENING  021-000   8482.09       0.00  0.194257     1     0\n",
              "4  ACCOPENING  021-004   3344.86       0.00  0.190937     1     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtqOkHYFW3p3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c95ff2c-42db-430a-a91f-e7153f5e0eed"
      },
      "source": [
        "dataNorm.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88623, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emmXfkbDW3qF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataNorm.values\n",
        "\n",
        "X = dataset[:, [0,1,5,6]]\n",
        "# format all fields as string\n",
        "X = X.astype(str)\n",
        "\n",
        "# y_train = dataset[:, [6]]\n",
        "# y_train = y_train.astype(int)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dQYapA8W3qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_enc, le = prepare_inputsMoj(X)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCztZhxTW3qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_encProba = np.array(X_enc).transpose()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUERIIjyW3ql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "04bfe3ec-f520-44a6-d438-14786e890b73"
      },
      "source": [
        "X_encProba = X_encProba.astype(float)\n",
        "X_encProba"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2.,   0.,   1.,   0.],\n",
              "       [  2.,   1.,   0.,   1.],\n",
              "       [  2.,   2.,   1.,   0.],\n",
              "       ...,\n",
              "       [ 28., 181.,   1.,   0.],\n",
              "       [ 28., 182.,   0.,   1.],\n",
              "       [ 28., 183.,   0.,   1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC8uXuNMGXLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6f92759-3013-4fd0-a3a7-9eedcc35f3a3"
      },
      "source": [
        "X_encProba.size"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354492"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFLrjXWAHH53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f817a916-1ef6-49fc-cb81-a0b9d0c42a0e"
      },
      "source": [
        "X_encProba[:,:].shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88623, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "igYgJ-mGW3qr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "55c81dd9-5c3b-45aa-8285-1b0cea795174"
      },
      "source": [
        "\n",
        "'''\n",
        "X_encProbaTrain = X_encProba[0:78638,:]#[0:44819,:]\n",
        "print(\"2007-2017- train\", X_encProbaTrain.shape)\n",
        "X_encProbaValidation = X_encProba[78638:84759,:]#[44819:69303,:]\n",
        "print(\"2018- validation\", X_encProbaValidation.shape)\n",
        "X_encProbaTest = X_encProba[84759:,:]\n",
        "print(\"2019- test\", X_encProbaTest.shape)\n",
        "\n",
        "X_encProbaTrain = X_encProba[0:70000,:]#[0:44819,:]\n",
        "print(\"2007-2017- train\", X_encProbaTrain.shape)\n",
        "X_encProbaValidation = X_encProba[70000:76000,:]#[44819:69303,:]\n",
        "print(\"2018- validation\", X_encProbaValidation.shape)\n",
        "X_encProbaTest = X_encProba[76000:80000,:]\n",
        "print(\"2019- test\", X_encProbaTest.shape)\n",
        "'''\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "#VRLO VAZNO I KONACNO: TRAIN I VALIDATE TREBAJU BITI OKRUGAO BROJ DJELJIV S BATCH_SIZE\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "dfVelicinaTrain = dfVelicinaTrain-38\n",
        "X_encProbaTrain = X_encProba[0:dfVelicinaTrain,:]#[0:44819,:]\n",
        "print(\"2007-2017- train\", X_encProbaTrain.shape)\n",
        "X_encProbaValidation = X_encProba[dfVelicinaTrain:dfVelicinaTrain+dfVelicinaValidate - 64,:]    #[44819:69303,:]\n",
        "print(\"2018- validation\", X_encProbaValidation.shape)\n",
        "X_encProbaTest = X_encProba[dfVelicinaTrain+dfVelicinaValidate:dfVelicinaTrain+dfVelicinaValidate+dfVelicinaTest,:]\n",
        "print(\"2019- test\", X_encProbaTest.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2007-2017- train (78600, 4)\n",
            "2018- validation (6057, 4)\n",
            "2019- test (3864, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFO5wv5VW3q3",
        "colab_type": "text"
      },
      "source": [
        "##################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enpSpwPvW3q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BASE_DIR = 'C:/Users/gianc/Desktop/PhD/Progetti/vae/'\n",
        "#TRAIN_DATA_FILE = BASE_DIR + 'train.csv'#'train_micro.csv'\n",
        "#GLOVE_EMBEDDING = BASE_DIR + 'glove.6B.300d.txt'\n",
        "#VALIDATION_SPLIT = 0.2\n",
        "MAX_SEQUENCE_LENGTH = 4 #2 #koliko trebam kolona orig 25\n",
        "batch_size = 100\n",
        "#NB_WORDS = NB_WORDS  #ovo je vocabular size odnosno kod mene koliko ima unikatnih, ne smije biti manji\n",
        "MAX_NB_WORDS = NB_WORDS = X_encProbaTrain.shape[0] - batch_size #100000 #88623 \n",
        "EMBEDDING_DIM = 50\n",
        "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN9BUO4NW3rE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89e9a9d9-6ef4-41f0-add1-bd31c00b3908"
      },
      "source": [
        "glove_embedding_matrix.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78500, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUO5Kc7QELtS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8323096c-bd52-4fee-d871-7cbcfdfab87b"
      },
      "source": [
        "NB_WORDS"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbowd-ymK1Dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5d85f16d-a357-4d37-f1a2-354846e58a32"
      },
      "source": [
        "#!pip install tensorflow==2.2.0\n",
        "import tensorflow as tf\n",
        "print('Morao sam downgrade na verziju 2.2.0 a aktualna je verzija 2.3.0.Dakle trenutn verzija je ' + tf.__version__)\n",
        "#!pip uninstall tensorflow==1.13.1\n",
        "#!pip install tensorflow_addons\n",
        "#ovo je za tfa.seq2seq koji je selio s tf.contrib u posebne addons-e\n",
        "import tensorflow_addons as tfa\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Morao sam downgrade na verziju 2.2.0 a aktualna je verzija 2.3.0...2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oS9HsqUoW3rK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "d49abdb0-4d2b-4ea0-f06f-258071186212"
      },
      "source": [
        "\n",
        "max_len = 4 #2 #DUZINA RECENICE MAKSIMALNA, kod mene je to broj kolona\n",
        "emb_dim = 50\n",
        "latent_dim = 30\n",
        "intermediate_dim = 80\n",
        "epsilon_std = 1.0\n",
        "kl_weight = 0.01\n",
        "#num_sampled=8011\n",
        "#act = ELU()\n",
        "\n",
        "\n",
        "x = Input(shape=(max_len,))\n",
        "\n",
        "# x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n",
        "#                             input_length=max_len, trainable=False)(x)\n",
        "\n",
        "x_embed = Embedding(NB_WORDS, emb_dim,\n",
        "                            input_length=max_len, trainable=False)(x)\n",
        "\n",
        "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2),\n",
        "                  merge_mode='concat', name = 'lstmPrvi')(x_embed)\n",
        "\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
        "                              stddev=epsilon_std)\n",
        "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "# we instantiate these layers separately so as to reuse them later\n",
        "repeated_context = RepeatVector(max_len)\n",
        "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2, name='lstmDrugi')\n",
        "decoder_mean = Dense(NB_WORDS, activation='linear')#softmax is applied in the seq2seqloss by tf #TimeDistributed()\n",
        "h_decoded = decoder_h(repeated_context(z))\n",
        "x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "\n",
        "# placeholder loss\n",
        "def zero_loss(y_true, y_pred):\n",
        "    return K.zeros_like(y_pred)\n",
        "\n",
        "\n",
        "# Custom loss layer\n",
        "class CustomVariationalLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
        "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
        "\n",
        "    def vae_loss(self, x, x_decoded_mean):\n",
        "        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n",
        "        labels = tf.cast(x, tf.int32)\n",
        "        #tfa je zamijenio tf.contrib u tensorflow 2.2\n",
        "        xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
        "                                                     weights=self.target_weights,\n",
        "                                                     average_across_timesteps=False,\n",
        "                                                     average_across_batch=False), axis=-1)#,\n",
        "                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#,\n",
        "        \n",
        "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        xent_loss = K.mean(xent_loss)\n",
        "        kl_loss = K.mean(kl_loss)\n",
        "        return K.mean(xent_loss + kl_weight * kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        x_decoded_mean = inputs[1]\n",
        "        print(x.shape, x_decoded_mean.shape)\n",
        "        loss = self.vae_loss(x, x_decoded_mean)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        # we don't use this output, but it has to have the correct shape:\n",
        "        return K.ones_like(x)\n",
        "    \n",
        "def kl_loss(x, x_decoded_mean):\n",
        "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    kl_loss = kl_weight * kl_loss\n",
        "    return kl_loss\n",
        "\n",
        "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
        "vae = Model(x, [loss_layer])\n",
        "\n",
        "opt = Adam(lr=0.001, decay=1e-6) \n",
        "vae.compile(optimizer=opt, loss=[zero_loss], metrics=[kl_loss])\n",
        "vae.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 4) (100, 4, 78500)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 4, 50)        3925000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstmPrvi (Bidirectional)        (None, 160)          83840       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 30)           4830        lstmPrvi[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 30)           4830        lstmPrvi[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (100, 30)            0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (100, 4, 30)         0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstmDrugi (LSTM)                (100, 4, 80)         35520       repeat_vector[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (100, 4, 78500)      6358500     lstmDrugi[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "custom_variational_layer (Custo (None, 4)            0           input_1[0][0]                    \n",
            "                                                                 dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 10,412,520\n",
            "Trainable params: 6,487,520\n",
            "Non-trainable params: 3,925,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNrrNcwTW3sa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Model training\n",
        "\n",
        "We train our model for 100 epochs through keras \".fit()\". For validation data we pass the same array twice since input and labels of this model are the same. If we didn't use the \"tf.contrib.seq2seq.sequence_loss\" (or another similar function) we would have had to pass as labels the sequence of word one-hot encodings with dimension (batch_size, seq_len, vocab_size) consuming a lot of memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcX0jFo9W3sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_checkpoint(dir, model_name):\n",
        "    #filepath = dir + '/' + model_name + \".h5\" \n",
        "    filepath = '/content/drive/My Drive/Colab Notebooks/'+dir+model_name+'.h5'\n",
        "    directory = os.path.dirname(filepath)\n",
        "    try:\n",
        "        os.stat(directory)\n",
        "    except:\n",
        "        os.mkdir(directory)\n",
        "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
        "    return checkpointer\n",
        "\n",
        "checkpointer = create_model_checkpoint('data/', 'vae_seq2seq_test_very_high_std')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NRhHMlsDW3sj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "d793a157-13c7-41df-b6a2-28be38b84932"
      },
      "source": [
        "\n",
        "vae.fit(X_encProbaTrain, X_encProbaTrain,\n",
        "     shuffle=True,\n",
        "     epochs=100,\n",
        "     batch_size=batch_size,\n",
        "     validation_split=0.1)\n",
        "     #validation_data=(X_encProbaValidation, X_encProbaValidation), callbacks=[checkpointer])\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "(None, 4) (100, 4, 78500)\n",
            "(None, 4) (100, 4, 78500)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "_SymbolicException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: dense_1/Identity:0",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31m_SymbolicException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cebc2fa99c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m      validation_split=0.1)\n\u001b[0m\u001b[1;32m      7\u001b[0m      \u001b[0;31m#validation_data=(X_encProbaValidation, X_encProbaValidation), callbacks=[checkpointer])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m       raise core._SymbolicException(\n\u001b[1;32m     73\u001b[0m           \u001b[0;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_SymbolicException\u001b[0m: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'dense_1/Identity:0' shape=(None, 30) dtype=float32>, <tf.Tensor 'dense/Identity:0' shape=(None, 30) dtype=float32>]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtkBuvyRoMpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = '/content/drive/My Drive/Colab Notebooks/data/'\n",
        "vae.save(filename+'vae_lstmExpanded.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gjzQs7W3sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "vae.load_weights(filename+'vae_lstmExpanded.h5')\n",
        "#ili ovo vae_lstmExpanded koji sam snimio, to je isto\n",
        "#vae.load_weights('./data/vae_lstmExpanded.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdwM9EmgW3s8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loss = vae.history.history['val_loss']\n",
        "kl_loss = vae.history.history['kl_loss']\n",
        "val_kl_loss = vae.history.history['val_kl_loss']\n",
        "loss = vae.history.history['loss']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(loss)\n",
        "plt.plot(val_loss)\n",
        "plt.plot(kl_loss)\n",
        "plt.plot(val_kl_loss)\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# plt.tight_layout()\n",
        "plt.savefig(filename+\"train.png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HPF8YwoW3tJ",
        "colab_type": "text"
      },
      "source": [
        "# Kraj"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faKY5oTves-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################################\n",
        "from sklearn.manifold.t_sne import TSNE\n",
        "\n",
        "#crtanje AKO THE ZEZA ERROR:\n",
        "#ImportError: No module named '_tkinter', please install the python3-tk package\n",
        "#ucitaj ovo:\n",
        "import matplotlib\n",
        "matplotlib.use(\"agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "\n",
        "# import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSVBHUXQezTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tsne = TSNE(n_components=2, random_state=0, learning_rate=500, metric='hamming',\n",
        "            perplexity=40, n_iter=5000) \n",
        "\n",
        "\n",
        "x_test = X_encProbaTest\n",
        "\n",
        "tsne_objTest = tsne.fit_transform(x_test[:,:])\n",
        "\n",
        "\n",
        "tsne_dfTest = pd.DataFrame({'X':tsne_objTest[:,0], 'Y':tsne_objTest[:,1]})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac7CDP5Xe3rT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#########################################################################################################################\n",
        "\n",
        "width = 40\n",
        "height = 40\n",
        "idAnomalije=[] # new array\n",
        "plt.figure(figsize=(width,height))\n",
        "\n",
        "y_test =  [0] * (x_test.shape[0])\n",
        "y_test[0] = 1\n",
        "colors = ['red','black','blue','green']\n",
        "\n",
        "plt.scatter(tsne_objTest[:,0], tsne_objTest[:,1], s=3900, cmap=matplotlib.colors.ListedColormap(colors), alpha=0.050)\n",
        "\n",
        "\n",
        "#cb = plt.colorbar()\n",
        "#loc = np.arange(0,max(y_test),max(y_test)/float(len(colors)))\n",
        "#cb.set_ticks(loc)\n",
        "#cb.set_ticklabels(colors)\n",
        "\n",
        "\n",
        "\n",
        "plt.savefig(filename+\"tsneReal.png\")\n",
        "\n",
        "#########################################################################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH6hHXjze7xO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlCCRipcW3tK",
        "colab_type": "raw"
      },
      "source": [
        "## Results\n",
        "\n",
        "After training with these parameters for 100 epochs I got these results from interpolating between these two sentences:\n",
        "\n",
        "sentence1=[‘where can i find a book on machine learning’] sentence2=[‘how can i become a successful entrepreneur’]\n",
        "\n",
        "Generated sentences:\n",
        "\n",
        "    where can i find a book on machine learning\n",
        "    where can i find a a machine book\n",
        "    how can i write a a machine book\n",
        "    how can i become a successful architect\n",
        "    how can i become a successful architect\n",
        "    how can i become a successful entrepreneur\n",
        "\n",
        "As we can see the results are not yet completely satisfying because not all the sentences are grammatically correct and in the interpolation the same sentence has been generated multiple times but anyway the model, even in this preliminary version seems to start working. There are certainly many improvements that could be done like:\n",
        "\n",
        "    removing all the sentences longer than 15 instead of just truncating them\n",
        "    introduce KL term annealing\n",
        "    introduce the equivalent of word dropout used in the original paper for this decoder architecture\n",
        "    parameter tuning (this model trains in few hours on a GTX950M with 2GB memory so it’s definitely possible to try larger nets)\n",
        "    using word embeddings with higher dimensionality\n",
        "    train on a more general dataset (Quora sentences are all questions)\n",
        "\n",
        "Stay tuned for future refinings of the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWZchBo8eqMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}